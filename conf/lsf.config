/*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    LSF executor configuration for DTU HPC
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Nextflow submits each process as a separate bsub job.
    Parallel processes run on different nodes simultaneously.
----------------------------------------------------------------------------------------
*/

process {
    executor = 'lsf'
    queue    = 'hpc'

    // NF 25.10.4 does not divide rusage[mem=X] by CPUs (only -M is divided, not rusage).
    // The auto-generated directive is always rusage[mem=<full task.memory>], which for a
    // 16-CPU / 40 GB job produces rusage[mem=40960].  LSF then tries to reserve 40 GB
    // per slot × 16 slots = 640 GB on one host — impossible on 128 GB nodes.
    //
    // Fix: append a correctly-divided rusage after the auto-generated one.
    // IBM LSF merges multiple -R strings and the LAST rusage specification wins,
    // so this overrides the auto-generated undivided value.
    // For UNICYCLER (16 CPUs, 40 GB): overrides rusage[mem=40960] with rusage[mem=2560]
    // → 16 × 2560 MB = 40 GB total reservation, schedulable on any 128 GB node.
    clusterOptions = { "-R \"rusage[mem=${task.memory.toMega().intdiv(task.cpus)}]\"" }
}

// How many jobs Nextflow can submit at once
executor {
    name              = 'lsf'
    queueSize         = 20
    submitRateLimit   = '25/1min'
    pollInterval      = '30 sec'
    queueStatInterval = '5 min'
    // perJobMemLimit = false → divides -M (kill limit) by CPUs.
    // NOTE: in NF 25.10.4 this does NOT divide rusage[mem=X] — rusage is always set
    // to full task.memory.  The clusterOptions closure in the process block above
    // handles the rusage division via LSF's last-wins merge rule.
    perJobMemLimit = false
}
