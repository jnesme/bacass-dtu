/*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    LSF executor configuration for DTU HPC
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Nextflow submits each process as a separate bsub job.
    Parallel processes run on different nodes simultaneously.
----------------------------------------------------------------------------------------
*/

process {
    executor = 'lsf'
    queue    = 'hpc'

    // Each pipeline task is a single process — no need for span[hosts=1]
    // since LSF places single-host jobs by default.
    // Nextflow auto-generates -R "rusage[mem=X]" from the memory directive.
    // Adding a second -R for span would cause "Multiple -R not supported" errors.
}

// How many jobs Nextflow can submit at once
executor {
    name              = 'lsf'
    queueSize         = 20
    submitRateLimit   = '25/1min'
    pollInterval      = '30 sec'
    queueStatInterval = '5 min'
    // perJobMemLimit = false (default) → Nextflow divides total memory by CPUs before
    // passing to LSF rusage[mem=X], so LSF sees per-slot memory (correct for DTU HPC).
    // Do NOT set perJobMemLimit = true — that disables the division, causing LSF to
    // interpret the full job memory as per-slot and multiply by CPU count (e.g.
    // 16 CPUs × 80 GB = 1280 GB reservation, which exceeds queue limits and node sizes).
}
