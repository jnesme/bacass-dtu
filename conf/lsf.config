/*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    LSF executor configuration for DTU HPC
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Nextflow submits each process as a separate bsub job.
    Parallel processes run on different nodes simultaneously.
----------------------------------------------------------------------------------------
*/

process {
    executor = 'lsf'
    queue    = 'hpc'
}

executor {
    name          = 'lsf'
    queueSize     = 20
    // pollInterval: how often NF polls bjobs to check job status.
    // HPC support (A. Bordoni) advised against short intervals — each poll is a bjobs
    // call that loads the LSF scheduler daemon.  5 min is a reasonable trade-off.
    pollInterval  = '5 min'
    // perJobMemLimit = false → NF divides -M (kill limit) by CPUs (per-slot kill limit).
    // DTU HPC lsf.conf has LSB_JOB_MEMLIMIT=Y which NF auto-detects as perJobMemLimit=true
    // (disabling -M division). setup.sh shadows lsf.conf with LSB_JOB_MEMLIMIT=N to fix this.
    perJobMemLimit = false
    // perTaskReserve = true → NF divides rusage[mem=X] by CPUs, generating a single clean
    // -R "select[mem>=<total>] rusage[mem=<per-slot>]" string.
    // DTU HPC lsf.conf does not set RESOURCE_RESERVE_PER_TASK, so we set it explicitly.
    // This replaces the previous clusterOptions last-wins workaround.
    perTaskReserve = true
}
