/*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    LSF executor configuration for DTU HPC
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Nextflow submits each process as a separate bsub job.
    Parallel processes run on different nodes simultaneously.
----------------------------------------------------------------------------------------
*/

process {
    executor = 'lsf'
    queue    = 'hpc'

    // Each pipeline task is a single process — no need for span[hosts=1]
    // since LSF places single-host jobs by default.
    // Nextflow auto-generates -R "rusage[mem=X]" from the memory directive.
    // Adding a second -R for span would cause "Multiple -R not supported" errors.
}

// How many jobs Nextflow can submit at once
executor {
    name              = 'lsf'
    queueSize         = 20
    submitRateLimit   = '25/1min'
    pollInterval      = '30 sec'
    queueStatInterval = '5 min'
    // perJobMemLimit = false → Nextflow divides total memory by CPUs before passing to
    // LSF rusage[mem=X], so LSF sees per-slot memory (correct for DTU HPC).
    // Must be set explicitly: in NF 25.x the default appears to be true (no division),
    // which causes LSF to multiply per-slot × CPUs (e.g. 16 CPUs × 80 GB = 1280 GB),
    // exceeding queue limits and preventing scheduling.
    perJobMemLimit = false
}
